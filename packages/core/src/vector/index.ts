/**
 * Vector Database Manager
 * Manages embeddings and semantic search using Qdrant
 */

import { QdrantClient } from '@qdrant/js-client-rest';
import OpenAI from 'openai';
import {
  VectorSearchResult,
  CodeChunkPayload,
  DocstringPayload,
  CommitPayload,
  VectorError,
  CodeChunk,
  VectorPayload
} from '@cv-git/shared';
import { chunkArray } from '@cv-git/shared';

export interface VectorCollections {
  codeChunks: string;
  docstrings: string;
  commits: string;
}

// Embedding model configurations with their vector dimensions
const EMBEDDING_MODELS: Record<string, { dimension: number; provider: 'openai' | 'openrouter' | 'ollama' }> = {
  // OpenAI models (direct)
  'text-embedding-3-small': { dimension: 1536, provider: 'openai' },
  'text-embedding-3-large': { dimension: 3072, provider: 'openai' },
  'text-embedding-ada-002': { dimension: 1536, provider: 'openai' },
  // OpenRouter models (uses OpenAI-compatible API)
  'openai/text-embedding-3-small': { dimension: 1536, provider: 'openrouter' },
  'openai/text-embedding-3-large': { dimension: 3072, provider: 'openrouter' },
  'openai/text-embedding-ada-002': { dimension: 1536, provider: 'openrouter' },
  // Ollama models (local)
  'nomic-embed-text': { dimension: 768, provider: 'ollama' },
  'mxbai-embed-large': { dimension: 1024, provider: 'ollama' },
  'all-minilm': { dimension: 384, provider: 'ollama' },
  'snowflake-arctic-embed': { dimension: 1024, provider: 'ollama' }
};

// Model fallback order for OpenRouter (preferred)
const OPENROUTER_MODEL_ORDER = [
  'openai/text-embedding-3-small',
  'openai/text-embedding-ada-002',
  'openai/text-embedding-3-large'
];

// Model fallback order for direct OpenAI (if OpenRouter unavailable)
const OPENAI_MODEL_ORDER = [
  'text-embedding-3-small',
  'text-embedding-ada-002'
];

// Ollama fallback order
const OLLAMA_MODEL_ORDER = [
  'nomic-embed-text',
  'mxbai-embed-large',
  'all-minilm'
];

export interface VectorManagerOptions {
  /** Qdrant URL */
  url: string;
  /** OpenRouter API key (preferred for embeddings) */
  openrouterApiKey?: string;
  /** OpenAI API key (fallback) */
  openaiApiKey?: string;
  /** Collection names */
  collections?: Partial<VectorCollections>;
  /** Embedding model to use */
  embeddingModel?: string;
  /** Ollama URL for local embeddings */
  ollamaUrl?: string;
}

export class VectorManager {
  private client: QdrantClient | null = null;
  private openai: OpenAI | null = null;
  private openrouter: OpenAI | null = null;
  private collections: VectorCollections;
  private embeddingModel: string;
  private embeddingProvider: 'openai' | 'openrouter' | 'ollama';
  private ollamaUrl: string;
  private openrouterApiKey?: string;
  private openaiApiKey?: string;
  private vectorSize: number;
  private connected: boolean = false;
  private modelValidated: boolean = false;
  private url: string;

  constructor(options: VectorManagerOptions);
  /** @deprecated Use options object instead */
  constructor(url: string, openaiApiKey?: string, collections?: Partial<VectorCollections>, embeddingModel?: string);
  constructor(
    urlOrOptions: string | VectorManagerOptions,
    openaiApiKey?: string,
    collections?: Partial<VectorCollections>,
    embeddingModel?: string
  ) {
    // Handle both old and new constructor signatures
    let opts: VectorManagerOptions;
    if (typeof urlOrOptions === 'string') {
      // Legacy constructor
      opts = {
        url: urlOrOptions,
        openaiApiKey,
        collections,
        embeddingModel,
        openrouterApiKey: process.env.OPENROUTER_API_KEY
      };
    } else {
      opts = urlOrOptions;
    }

    this.url = opts.url;
    this.openaiApiKey = opts.openaiApiKey;
    this.openrouterApiKey = opts.openrouterApiKey || process.env.OPENROUTER_API_KEY;
    this.ollamaUrl = opts.ollamaUrl || process.env.OLLAMA_URL || 'http://localhost:11434';

    this.collections = {
      codeChunks: opts.collections?.codeChunks || 'code_chunks',
      docstrings: opts.collections?.docstrings || 'docstrings',
      commits: opts.collections?.commits || 'commits'
    };

    // Default to OpenRouter model if OpenRouter key available, otherwise OpenAI
    const defaultModel = this.openrouterApiKey
      ? 'openai/text-embedding-3-small'
      : 'text-embedding-3-small';

    this.embeddingModel = opts.embeddingModel || process.env.CV_EMBEDDING_MODEL || defaultModel;

    // Determine provider from model name or available keys
    const modelConfig = EMBEDDING_MODELS[this.embeddingModel];
    if (modelConfig) {
      this.embeddingProvider = modelConfig.provider;
    } else if (this.openrouterApiKey) {
      this.embeddingProvider = 'openrouter';
    } else if (this.openaiApiKey) {
      this.embeddingProvider = 'openai';
    } else {
      this.embeddingProvider = 'ollama';
    }

    this.vectorSize = modelConfig?.dimension || 1536;
  }

  /**
   * Connect to Qdrant and initialize embedding provider
   * Provider priority: OpenRouter > OpenAI > Ollama
   */
  async connect(): Promise<void> {
    try {
      // Initialize Qdrant client
      this.client = new QdrantClient({ url: this.url });

      // Test connection
      await this.client.getCollections();

      // Initialize embedding provider based on what's available
      // Priority: OpenRouter > OpenAI > Ollama
      if (this.embeddingProvider === 'ollama') {
        // Explicit Ollama request
        await this.initOllama();
      } else if (this.openrouterApiKey) {
        // OpenRouter available - use it (preferred)
        this.openrouter = new OpenAI({
          apiKey: this.openrouterApiKey,
          baseURL: 'https://openrouter.ai/api/v1'
        });
        this.embeddingProvider = 'openrouter';
        // Use OpenRouter model naming
        if (!this.embeddingModel.includes('/')) {
          this.embeddingModel = `openai/${this.embeddingModel}`;
        }
      } else if (this.openaiApiKey) {
        // Fall back to OpenAI
        this.openai = new OpenAI({ apiKey: this.openaiApiKey });
        this.embeddingProvider = 'openai';
        // Use OpenAI model naming (strip openai/ prefix if present)
        if (this.embeddingModel.startsWith('openai/')) {
          this.embeddingModel = this.embeddingModel.replace('openai/', '');
        }
      } else {
        // No cloud API keys - try Ollama
        const ollamaAvailable = await this.isOllamaAvailable();
        if (ollamaAvailable) {
          await this.initOllama();
        } else {
          throw new VectorError(
            'No embedding API key provided.\n' +
            'Run: cv auth setup openrouter (recommended)\n' +
            'Or:  cv auth setup openai\n' +
            'Or:  Start Ollama for local embeddings'
          );
        }
      }

      this.connected = true;

      // Ensure collections exist
      await this.ensureCollections();

    } catch (error: any) {
      throw new VectorError(`Failed to connect to Qdrant: ${error.message}`, error);
    }
  }

  /**
   * Initialize Ollama and verify model availability
   */
  private async initOllama(): Promise<void> {
    // Check if Ollama is running
    try {
      const response = await fetch(`${this.ollamaUrl}/api/tags`, {
        signal: AbortSignal.timeout(5000)
      });

      if (!response.ok) {
        throw new Error('Ollama not responding');
      }

      const data = await response.json() as { models?: Array<{ name: string }> };
      const availableModels = data.models?.map(m => m.name.split(':')[0]) || [];

      // Check if our model is available, try fallbacks
      let modelFound = false;
      for (const model of [this.embeddingModel, ...OLLAMA_MODEL_ORDER]) {
        if (availableModels.some(m => m === model || m.startsWith(model))) {
          if (model !== this.embeddingModel) {
            console.log(`Ollama: Using ${model} (${this.embeddingModel} not found)`);
            this.embeddingModel = model;
            const modelConfig = EMBEDDING_MODELS[model];
            if (modelConfig) {
              this.vectorSize = modelConfig.dimension;
            }
          }
          modelFound = true;
          break;
        }
      }

      if (!modelFound) {
        throw new Error(
          `No embedding model found in Ollama. Install one with: ollama pull nomic-embed-text`
        );
      }

    } catch (error: any) {
      if (error.message.includes('fetch failed') || error.message.includes('ECONNREFUSED')) {
        throw new VectorError(
          'Ollama not running. Start with: ollama serve\n' +
          'Or install: curl -fsSL https://ollama.com/install.sh | sh'
        );
      }
      throw new VectorError(`Failed to initialize Ollama: ${error.message}`, error);
    }
  }

  /**
   * Generate embedding using Ollama
   */
  private async embedWithOllama(text: string): Promise<number[]> {
    const response = await fetch(`${this.ollamaUrl}/api/embeddings`, {
      method: 'POST',
      headers: { 'Content-Type': 'application/json' },
      body: JSON.stringify({
        model: this.embeddingModel,
        prompt: text
      }),
      signal: AbortSignal.timeout(30000)
    });

    if (!response.ok) {
      const error = await response.text();
      throw new Error(`Ollama embedding failed: ${error}`);
    }

    const data = await response.json() as { embedding: number[] };
    return data.embedding;
  }

  /**
   * Generate embeddings for multiple texts using Ollama (sequential)
   */
  private async embedBatchWithOllama(texts: string[]): Promise<number[][]> {
    const embeddings: number[][] = [];
    for (const text of texts) {
      const embedding = await this.embedWithOllama(text);
      embeddings.push(embedding);
    }
    return embeddings;
  }

  /**
   * Generate embeddings using OpenRouter (OpenAI-compatible API)
   */
  private async embedWithOpenRouter(input: string | string[]): Promise<{ embeddings: number[][]; model: string }> {
    if (!this.openrouter) {
      throw new VectorError('OpenRouter client not initialized');
    }

    try {
      const response = await this.openrouter.embeddings.create({
        model: this.embeddingModel,
        input,
        encoding_format: 'float'
      });

      return {
        embeddings: response.data.map(d => d.embedding),
        model: this.embeddingModel
      };
    } catch (error: any) {
      throw new VectorError(`OpenRouter embedding failed: ${error.message}`, error);
    }
  }

  /**
   * Ensure all collections exist
   */
  private async ensureCollections(): Promise<void> {
    if (!this.client) {
      throw new VectorError('Not connected to Qdrant');
    }

    // Create code chunks collection
    await this.ensureCollection(this.collections.codeChunks, this.vectorSize);

    // Create docstrings collection
    await this.ensureCollection(this.collections.docstrings, this.vectorSize);

    // Create commits collection
    await this.ensureCollection(this.collections.commits, this.vectorSize);
  }

  /**
   * Create collection if not exists
   */
  async ensureCollection(name: string, vectorSize: number): Promise<void> {
    if (!this.client) {
      throw new VectorError('Not connected to Qdrant');
    }

    try {
      // Check if collection exists
      const collections = await this.client.getCollections();
      const exists = collections.collections.some(c => c.name === name);

      if (!exists) {
        // Create collection
        await this.client.createCollection(name, {
          vectors: {
            size: vectorSize,
            distance: 'Cosine'
          }
        });
      }
    } catch (error: any) {
      throw new VectorError(`Failed to ensure collection ${name}: ${error.message}`, error);
    }
  }

  /**
   * Generate embedding for text
   */
  async embed(text: string): Promise<number[]> {
    // Use the appropriate provider
    if (this.embeddingProvider === 'ollama') {
      return this.embedWithOllama(text);
    }

    if (this.embeddingProvider === 'openrouter') {
      if (!this.openrouter) {
        throw new VectorError('OpenRouter client not initialized');
      }
      const result = await this.embedWithOpenRouter(text);
      return result.embeddings[0];
    }

    // OpenAI direct
    if (!this.openai) {
      throw new VectorError('OpenAI client not initialized');
    }

    try {
      const response = await this.openai.embeddings.create({
        model: this.embeddingModel,
        input: text,
        encoding_format: 'float'
      });

      return response.data[0].embedding;
    } catch (error: any) {
      throw new VectorError(`Failed to generate embedding: ${error.message}`, error);
    }
  }

  /**
   * Check if Ollama is available
   */
  private async isOllamaAvailable(): Promise<boolean> {
    try {
      const response = await fetch(`${this.ollamaUrl}/api/tags`, {
        signal: AbortSignal.timeout(2000)
      });
      return response.ok;
    } catch {
      return false;
    }
  }

  /**
   * Try to generate embeddings with automatic model fallback (including OpenRouter and Ollama)
   */
  private async tryEmbeddingWithFallback(input: string | string[]): Promise<{ embeddings: number[][]; model: string }> {
    // If using Ollama provider, use it directly
    if (this.embeddingProvider === 'ollama') {
      const texts = Array.isArray(input) ? input : [input];
      const embeddings = await this.embedBatchWithOllama(texts);
      return { embeddings, model: this.embeddingModel };
    }

    // If using OpenRouter provider, use it directly
    if (this.embeddingProvider === 'openrouter') {
      return this.embedWithOpenRouter(input);
    }

    if (!this.openai) {
      throw new VectorError('OpenAI client not initialized');
    }

    // If model already validated, use it directly
    if (this.modelValidated) {
      const response = await this.openai.embeddings.create({
        model: this.embeddingModel,
        input,
        encoding_format: 'float'
      });
      return {
        embeddings: response.data.map(d => d.embedding),
        model: this.embeddingModel
      };
    }

    // Try models in fallback order
    const modelsToTry = this.embeddingModel === OPENAI_MODEL_ORDER[0]
      ? OPENAI_MODEL_ORDER
      : [this.embeddingModel, ...OPENAI_MODEL_ORDER.filter(m => m !== this.embeddingModel)];

    let lastError: Error | null = null;
    let allOpenAIFailed = true;

    for (const model of modelsToTry) {
      try {
        const response = await this.openai.embeddings.create({
          model,
          input,
          encoding_format: 'float'
        });

        // Model works! Update settings
        if (model !== this.embeddingModel) {
          console.log(`Switched to embedding model: ${model}`);
          this.embeddingModel = model;
          this.vectorSize = EMBEDDING_MODELS[model]?.dimension || 1536;
        }
        this.modelValidated = true;
        allOpenAIFailed = false;

        return {
          embeddings: response.data.map(d => d.embedding),
          model
        };
      } catch (error: any) {
        lastError = error;
        // Check if it's an access/permission error (403)
        if (error.status === 403 || error.message?.includes('403') || error.message?.includes('does not have access')) {
          console.log(`Model ${model} not accessible, trying fallback...`);
          continue;
        }
        // Other errors should be thrown immediately
        throw error;
      }
    }

    // All OpenAI models failed with 403 - try OpenRouter first if available
    if (allOpenAIFailed && this.openrouterApiKey) {
      console.log('OpenAI models not accessible. Trying OpenRouter as fallback...');
      try {
        this.openrouter = new OpenAI({
          apiKey: this.openrouterApiKey,
          baseURL: 'https://openrouter.ai/api/v1'
        });
        this.embeddingProvider = 'openrouter';
        this.embeddingModel = 'openai/text-embedding-3-small';
        this.vectorSize = 1536;
        const result = await this.embedWithOpenRouter(input);
        this.modelValidated = true;
        return result;
      } catch (openrouterError: any) {
        console.log(`OpenRouter fallback failed: ${openrouterError.message}`);
      }
    }

    // Try Ollama as last resort
    if (allOpenAIFailed && await this.isOllamaAvailable()) {
      console.log('Trying Ollama as fallback...');
      try {
        await this.initOllama();
        this.embeddingProvider = 'ollama';
        const texts = Array.isArray(input) ? input : [input];
        const embeddings = await this.embedBatchWithOllama(texts);
        this.modelValidated = true;
        return { embeddings, model: this.embeddingModel };
      } catch (ollamaError: any) {
        console.log(`Ollama fallback failed: ${ollamaError.message}`);
      }
    }

    throw new VectorError(`Failed to generate embeddings with any model: ${lastError?.message}`, lastError);
  }

  /**
   * Generate embeddings for multiple texts in batches
   */
  async embedBatch(texts: string[]): Promise<number[][]> {
    // If using Ollama, use Ollama batch
    if (this.embeddingProvider === 'ollama') {
      return this.embedBatchWithOllama(texts);
    }

    // If using OpenRouter, use OpenRouter batch
    if (this.embeddingProvider === 'openrouter') {
      if (!this.openrouter) {
        throw new VectorError('OpenRouter client not initialized');
      }
      const batchSize = 100;
      const batches = chunkArray(texts, batchSize);
      const allEmbeddings: number[][] = [];

      for (const batch of batches) {
        const result = await this.embedWithOpenRouter(batch);
        allEmbeddings.push(...result.embeddings);
      }

      return allEmbeddings;
    }

    // Using OpenAI directly
    if (!this.openai) {
      throw new VectorError('OpenAI client not initialized');
    }

    try {
      // OpenAI allows up to 2048 inputs per request
      const batchSize = 100; // Use smaller batches to be safe
      const batches = chunkArray(texts, batchSize);
      const allEmbeddings: number[][] = [];

      for (const batch of batches) {
        const result = await this.tryEmbeddingWithFallback(batch);
        allEmbeddings.push(...result.embeddings);
      }

      return allEmbeddings;
    } catch (error: any) {
      throw new VectorError(`Failed to generate batch embeddings: ${error.message}`, error);
    }
  }

  /**
   * Upsert a vector into a collection
   */
  async upsert(
    collection: string,
    id: string,
    vector: number[],
    payload: any
  ): Promise<void> {
    if (!this.client) {
      throw new VectorError('Not connected to Qdrant');
    }

    try {
      await this.client.upsert(collection, {
        wait: true,
        points: [
          {
            id: this.hashId(id),
            vector,
            payload: { ...payload, _id: id }
          }
        ]
      });
    } catch (error: any) {
      throw new VectorError(`Failed to upsert vector: ${error.message}`, error);
    }
  }

  /**
   * Upsert multiple vectors in batch
   */
  async upsertBatch(
    collection: string,
    items: Array<{ id: string; vector: number[]; payload: any }>
  ): Promise<void> {
    if (!this.client) {
      throw new VectorError('Not connected to Qdrant');
    }

    try {
      // Batch upsert in chunks
      const batchSize = 100;
      const batches = chunkArray(items, batchSize);

      for (const batch of batches) {
        await this.client.upsert(collection, {
          wait: true,
          points: batch.map(item => ({
            id: this.hashId(item.id),
            vector: item.vector,
            payload: { ...item.payload, _id: item.id }
          }))
        });
      }
    } catch (error: any) {
      throw new VectorError(`Failed to batch upsert: ${error.message}`, error);
    }
  }

  /**
   * Search vectors by query text
   */
  async search<T extends VectorPayload = CodeChunkPayload>(
    collection: string,
    query: string,
    limit: number = 10,
    filter?: any
  ): Promise<VectorSearchResult<T>[]> {
    if (!this.client) {
      throw new VectorError('Not connected to Qdrant');
    }

    try {
      if (process.env.CV_DEBUG) {
        console.log(`[VectorManager] Searching collection '${collection}' for query: "${query.slice(0, 50)}..."`);
      }

      // Generate embedding for query
      const queryVector = await this.embed(query);

      if (process.env.CV_DEBUG) {
        console.log(`[VectorManager] Generated embedding of length ${queryVector.length}`);
      }

      // Search
      const results = await this.client.search(collection, {
        vector: queryVector,
        limit,
        filter,
        with_payload: true
      });

      if (process.env.CV_DEBUG) {
        console.log(`[VectorManager] Search returned ${results.length} raw results`);
        if (results.length > 0) {
          console.log(`[VectorManager] Top score: ${results[0].score.toFixed(4)}`);
        }
      }

      return results.map(result => ({
        id: result.payload?._id as string || String(result.id),
        score: result.score,
        payload: result.payload as T
      }));
    } catch (error: any) {
      if (process.env.CV_DEBUG) {
        console.error(`[VectorManager] Search error: ${error.message}`);
      }
      throw new VectorError(`Search failed: ${error.message}`, error);
    }
  }

  /**
   * Search code chunks
   */
  async searchCode(
    query: string,
    limit: number = 10,
    options?: {
      language?: string;
      file?: string;
      minScore?: number;
    }
  ): Promise<VectorSearchResult<CodeChunkPayload>[]> {
    const filter: any = {};

    if (options?.language) {
      filter.must = filter.must || [];
      filter.must.push({
        key: 'language',
        match: { value: options.language }
      });
    }

    if (options?.file) {
      filter.must = filter.must || [];
      filter.must.push({
        key: 'file',
        match: { value: options.file }
      });
    }

    const results = await this.search<CodeChunkPayload>(
      this.collections.codeChunks,
      query,
      limit,
      Object.keys(filter).length > 0 ? filter : undefined
    );

    // Filter by minimum score if specified
    if (options?.minScore !== undefined) {
      return results.filter(r => r.score >= options.minScore!);
    }

    return results;
  }

  /**
   * Delete vector by ID
   */
  async delete(collection: string, id: string): Promise<void> {
    if (!this.client) {
      throw new VectorError('Not connected to Qdrant');
    }

    try {
      await this.client.delete(collection, {
        wait: true,
        points: [this.hashId(id)]
      });
    } catch (error: any) {
      throw new VectorError(`Failed to delete vector: ${error.message}`, error);
    }
  }

  /**
   * Clear entire collection
   */
  async clearCollection(collection: string): Promise<void> {
    if (!this.client) {
      throw new VectorError('Not connected to Qdrant');
    }

    try {
      await this.client.deleteCollection(collection);
      await this.ensureCollection(collection, this.vectorSize);
    } catch (error: any) {
      throw new VectorError(`Failed to clear collection: ${error.message}`, error);
    }
  }

  /**
   * Get collection statistics
   */
  async getCollectionInfo(collection: string): Promise<any> {
    if (!this.client) {
      throw new VectorError('Not connected to Qdrant');
    }

    try {
      return await this.client.getCollection(collection);
    } catch (error: any) {
      throw new VectorError(`Failed to get collection info: ${error.message}`, error);
    }
  }

  /**
   * Scroll through all points in a collection
   * Used for exporting vectors to file storage
   */
  async scroll(
    collection: string,
    limit: number = 100,
    offset?: string
  ): Promise<{
    points: Array<{
      id: string | number;
      vector: number[];
      payload: Record<string, unknown>;
    }>;
    next_page_offset?: string;
  }> {
    if (!this.client) {
      throw new VectorError('Not connected to Qdrant');
    }

    try {
      // Qdrant scroll API: offset is a point ID (number or string)
      const scrollOptions: any = {
        limit,
        with_vector: true,
        with_payload: true
      };

      // Only set offset if provided (skip on first call)
      if (offset) {
        // Try to parse as number first, otherwise use string
        const parsedOffset = parseInt(offset, 10);
        scrollOptions.offset = isNaN(parsedOffset) ? offset : parsedOffset;
      }

      const result = await this.client.scroll(collection, scrollOptions);

      return {
        points: result.points.map(p => ({
          id: p.id,
          vector: p.vector as number[],
          payload: p.payload as Record<string, unknown>
        })),
        next_page_offset: result.next_page_offset != null ? String(result.next_page_offset) : undefined
      };
    } catch (error: any) {
      throw new VectorError(`Failed to scroll collection: ${error.message}`, error);
    }
  }

  /**
   * Close connection
   */
  async close(): Promise<void> {
    this.connected = false;
    this.client = null;
    this.openai = null;
  }

  /**
   * Check if connected
   */
  isConnected(): boolean {
    return this.connected;
  }

  /**
   * Hash string ID to numeric ID for Qdrant
   */
  private hashId(id: string): number {
    let hash = 0;
    for (let i = 0; i < id.length; i++) {
      const char = id.charCodeAt(i);
      hash = ((hash << 5) - hash) + char;
      hash = hash & hash; // Convert to 32-bit integer
    }
    return Math.abs(hash);
  }

  /**
   * Prepare code chunk text for embedding
   */
  prepareCodeForEmbedding(chunk: CodeChunk): string {
    const parts: string[] = [];

    // Add language context
    parts.push(`// Language: ${chunk.language}`);

    // Add file context
    parts.push(`// File: ${chunk.file}`);

    // Add symbol context if available
    if (chunk.symbolName) {
      parts.push(`// ${chunk.symbolKind}: ${chunk.symbolName}`);
    }

    // Add docstring if available
    if (chunk.docstring) {
      parts.push(`// ${chunk.docstring}`);
    }

    // Add the actual code
    parts.push('');
    parts.push(chunk.text);

    return parts.join('\n');
  }
}

/**
 * Create a VectorManager instance
 * @param options - VectorManagerOptions or legacy positional args
 */
export function createVectorManager(options: VectorManagerOptions): VectorManager;
/** @deprecated Use options object: createVectorManager({ url, openrouterApiKey, openaiApiKey, collections }) */
export function createVectorManager(url: string, openaiApiKey?: string, collections?: Partial<VectorCollections>): VectorManager;
export function createVectorManager(
  urlOrOptions: string | VectorManagerOptions,
  openaiApiKey?: string,
  collections?: Partial<VectorCollections>
): VectorManager {
  if (typeof urlOrOptions === 'string') {
    // Legacy signature - still works but OpenRouter will be preferred if OPENROUTER_API_KEY env var is set
    return new VectorManager(urlOrOptions, openaiApiKey, collections);
  }
  return new VectorManager(urlOrOptions);
}
